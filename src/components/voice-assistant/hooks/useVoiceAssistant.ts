import { useEffect, useRef, useState } from "react"
import { useRouter } from "next/navigation"
import { PorcupineWorker } from "@picovoice/porcupine-web"
import type { PorcupineWorker as PorcupineWorkerType } from "@picovoice/porcupine-web"

import { VoiceSettings, PendingAction, PorcupineDetection } from "../types"
import { blobToBase64, playBeep } from "../utils/audio"
import { sendVoiceCommand } from "../services/voiceCommandService"
import { pushNotificationService } from "@/services/push-notification.service"
import { groupMemberService } from "@/services/group-member.service"
import { eventEmitter } from "@/utils/event-emitter/event-emitter"
import { EInternalEvents } from "@/utils/event-emitter/events"
import {
  calculateSpeechProbability,
  createVADState,
  getVADThresholds,
  shouldStopRecording,
  updateVADState,
} from "../utils/vad"
import { handleClientAction } from "../utils/clientActions"
import { isConfirmation, parseSelectionIndex } from "../utils/confirmation"
import {
  handleCreateGroup,
  handleSendEmoji,
  handleSendMessage,
  handleSendSticker,
  handleMakeCall,
} from "../handlers"

export function useVoiceAssistant() {
  const router = useRouter()
  const [status, setStatus] = useState<string>("ƒêang t·∫£i c√†i ƒë·∫∑t...")
  const [isListening, setIsListening] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [settings, setSettings] = useState<VoiceSettings | null>(null)

  const pendingActionRef = useRef<PendingAction | null>(null)
  const lastAudioDataRef = useRef<string | null>(null)
  const isWaitingForConfirmationRef = useRef(false)
  const isWakeWordProcessingRef = useRef(false) // üîí Lock: ch·ªâ nh·∫≠n 1 l·ªánh sau m·ªói wake word detect
  const lastWakeWordDetectionTimeRef = useRef<number>(Date.now())
  const detectionHeartbeatRef = useRef<NodeJS.Timeout | null>(null)

  // Porcupine & Audio refs
  const workerRef = useRef<PorcupineWorkerType | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const micStreamRef = useRef<MediaStream | null>(null)
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const maxRecordingTimerRef = useRef<NodeJS.Timeout | null>(null)

  // Load settings on mount
  useEffect(() => {
    let mounted = true

    const loadSettings = async () => {
      try {
        const userSettings = {
          accessibility: {
            sttEnabled: true,
            voiceActivationMode: "WAKE_WORD",
            wakeWordPhrase: "Hey Chat",
            ttsEnabled: true,
            speechRate: 1.0,
          },
        }

        if (mounted) {
          setSettings({
            sttEnabled: userSettings.accessibility?.sttEnabled ?? true,
            voiceActivationMode:
              (userSettings.accessibility?.voiceActivationMode as "WAKE_WORD" | "LONG_PRESS") ??
              "WAKE_WORD",
            wakeWordPhrase: userSettings.accessibility?.wakeWordPhrase ?? "Hey Chat",
            ttsEnabled: userSettings.accessibility?.ttsEnabled ?? true,
            speechRate: userSettings.accessibility?.speechRate ?? 1.0,
          })

          console.log("‚úÖ Settings loaded")
          setStatus("‚úÖ ƒê√£ t·∫£i c√†i ƒë·∫∑t")
        }
      } catch (err) {
        console.error("‚ùå Kh√¥ng t·∫£i ƒë∆∞·ª£c settings:", err)
        if (mounted) {
          setSettings({
            sttEnabled: true,
            voiceActivationMode: "WAKE_WORD",
            wakeWordPhrase: "Hey Chat",
            ttsEnabled: true,
            speechRate: 1.0,
          })
          setStatus("‚ö†Ô∏è D√πng c√†i ƒë·∫∑t m·∫∑c ƒë·ªãnh")
        }
      }
    }

    loadSettings()

    // Track directChatId from events
    const handleFetchDirectChat = (chatId: number) => {
      console.log(`üîî Event listener tracking directChatId: ${chatId}`)
    }

    eventEmitter.on(EInternalEvents.FETCH_DIRECT_CHAT, handleFetchDirectChat)

    return () => {
      mounted = false
      eventEmitter.off(EInternalEvents.FETCH_DIRECT_CHAT, handleFetchDirectChat)
    }
  }, [])

  // Listen for incoming calls
  useEffect(() => {
    const handleIncomingCall = () => {
      console.log('üìû Nh·∫≠n cu·ªôc g·ªçi ƒë·∫øn - Ch·ªù "Hey Chat" ƒë·ªÉ ph·∫£n h·ªìi')
      pendingActionRef.current = {
        type: "incoming_call" as const,
        contactName: "Ng∆∞·ªùi g·ªçi",
        message: "",
      }
    }

    eventEmitter.on(EInternalEvents.VOICE_CALL_REQUEST_RECEIVED, handleIncomingCall)

    return () => {
      eventEmitter.off(EInternalEvents.VOICE_CALL_REQUEST_RECEIVED, handleIncomingCall)
    }
  }, [])

  // TTS function
  const speakText = async (
    text: string,
    rate?: number,
    waitForConfirmation?: boolean
  ): Promise<void> => {
    return new Promise((resolve) => {
      try {
        console.log("üîä Ph√°t TTS cho:", text)
        const utterance = new SpeechSynthesisUtterance(text)
        utterance.lang = "vi-VN"
        utterance.rate = Math.max(0.5, Math.min(2, rate ?? 1.0))
        utterance.pitch = 1.0
        utterance.volume = 1.0

        utterance.onend = () => {
          console.log("‚úÖ Ph√°t TTS xong ho√†n to√†n")
          if (waitForConfirmation) {
            console.log("‚è≥ Ch·ªù x√°c nh·∫≠n t·ª´ user...")
            setStatus('‚è≥ Ch·ªù x√°c nh·∫≠n... N√≥i "c√≥" ho·∫∑c "kh√¥ng"')
            isWaitingForConfirmationRef.current = true

            setTimeout(() => {
              console.log("üé§ B·∫Øt ƒë·∫ßu ghi √¢m x√°c nh·∫≠n")
              startRecordingAndSend()
            }, 500)
          } else {
            console.log("‚úÖ Ph√°t TTS xong - Ti·∫øp t·ª•c wake word detection")
            setStatus(`üéß ƒêang nghe "${settings?.wakeWordPhrase}"...`)
            isWaitingForConfirmationRef.current = false
            console.log("üé§ G·ªçi resumeWakeWordDetection() t·ª´ TTS onend")
            resumeWakeWordDetection().catch((err) => {
              console.error("‚ùå L·ªói ti·∫øp t·ª•c detection t·ª´ TTS:", err)
            })
          }
          resolve()
        }

        utterance.onerror = (event) => {
          console.error("‚ùå L·ªói TTS:", event.error)
          resolve()
        }

        window.speechSynthesis.speak(utterance)
      } catch (err) {
        console.error("‚ùå L·ªói TTS:", err)
        resolve()
      }
    })
  }

  // Cleanup function
  const cleanup = () => {
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((t) => t.stop())
      micStreamRef.current = null
    }
    if (workerRef.current) {
      workerRef.current.terminate()
      workerRef.current = null
    }
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
      silenceTimerRef.current = null
    }
    if (maxRecordingTimerRef.current) {
      clearTimeout(maxRecordingTimerRef.current)
      maxRecordingTimerRef.current = null
    }
    setIsListening(false)
  }

  // Start microphone for wake word detection
  const startMicrophoneForWakeWord = async (worker: PorcupineWorkerType) => {
    try {
      console.log("üéôÔ∏è ƒêang y√™u c·∫ßu quy·ªÅn microphone...")

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
        },
      })

      micStreamRef.current = stream
      console.log("‚úÖ Microphone stream ƒë√£ ƒë∆∞·ª£c c·∫•p")

      const audioContext = new AudioContext({ sampleRate: 16000 })
      audioContextRef.current = audioContext
      console.log("‚úÖ AudioContext ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o")

      const source = audioContext.createMediaStreamSource(stream)
      const processor = audioContext.createScriptProcessor(512, 1, 1)

      let processCount = 0
      processor.onaudioprocess = (event) => {
        lastWakeWordDetectionTimeRef.current = Date.now()

        const inputData = event.inputBuffer.getChannelData(0)

        let sum = 0
        for (let i = 0; i < inputData.length; i++) {
          sum += inputData[i] * inputData[i]
        }
        const rms = Math.sqrt(sum / inputData.length)

        if (processCount++ % 100 === 0) {
          console.log(`üéµ Audio processing... RMS: ${rms.toFixed(4)}, samples: ${inputData.length}`)
        }

        const pcm16 = new Int16Array(inputData.length)
        for (let i = 0; i < inputData.length; i++) {
          const s = Math.max(-1, Math.min(1, inputData[i]))
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7fff
        }

        try {
          worker.process(pcm16)
        } catch (err) {
          console.error("‚ùå L·ªói khi process audio:", err)
        }
      }

      source.connect(processor)
      processor.connect(audioContext.destination)

      setIsListening(true)
      setStatus(`üéß ƒêang nghe "${settings?.wakeWordPhrase}"...`)
      console.log("‚úÖ üéß Wake word detection ƒëang ho·∫°t ƒë·ªông")
    } catch (err) {
      console.error("‚ùå Kh√¥ng th·ªÉ truy c·∫≠p microphone:", err)
      setStatus("‚ùå Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c mic")
      throw err
    }
  }

  // Restart wake word detection
  const restartWakeWordDetection = async () => {
    try {
      console.log("üîÑ Kh·ªüi ƒë·ªông l·∫°i detection...")
      if (workerRef.current) {
        if (audioContextRef.current && audioContextRef.current.state === "running") {
          console.log("üîÑ ƒê√≥ng audioContext c≈©")
          await audioContextRef.current.close()
          audioContextRef.current = null
        }
        if (micStreamRef.current) {
          console.log("üîÑ D·ª´ng stream c≈©")
          micStreamRef.current.getTracks().forEach((t) => t.stop())
          micStreamRef.current = null
        }

        console.log("üîÑ Kh·ªüi ƒë·ªông stream microphone m·ªõi")
        await startMicrophoneForWakeWord(workerRef.current)
        console.log("üîÑ ‚úÖ Kh·ªüi ƒë·ªông detection th√†nh c√¥ng")
        setStatus(`üéß ƒêang nghe "${settings?.wakeWordPhrase}"...`)
      } else {
        console.log("üîÑ ‚ùå workerRef.current kh√¥ng t·ªìn t·∫°i!")
      }
    } catch (err) {
      console.error("‚ùå L·ªói kh·ªüi ƒë·ªông l·∫°i:", err)
    }
  }

  // Resume wake word detection (ti·∫øp t·ª•c l·∫Øng nghe m√† kh√¥ng kh·ªüi ƒë·ªông l·∫°i, d√πng khi stream v·∫´n c√≤n ch·∫°y)
  const resumeWakeWordDetection = async () => {
    try {
      console.log("‚ñ∂Ô∏è Ti·∫øp t·ª•c l·∫Øng nghe wake word...")
      if (workerRef.current && audioContextRef.current && micStreamRef.current) {
        console.log("‚ñ∂Ô∏è ‚úÖ Stream v·∫´n c√≤n ho·∫°t ƒë·ªông, ti·∫øp t·ª•c l·∫Øng nghe")
        setStatus(`üéß ƒêang nghe "${settings?.wakeWordPhrase}"...`)
      } else {
        console.log("‚ñ∂Ô∏è ‚ö†Ô∏è Stream kh√¥ng ho·∫°t ƒë·ªông, kh·ªüi ƒë·ªông l·∫°i...")
        await restartWakeWordDetection()
      }
    } catch (err) {
      console.error("‚ùå L·ªói ti·∫øp t·ª•c:", err)
    }
  }

  // Handle pending action confirmation
  const handlePendingActionConfirmation = async (transcript: string): Promise<boolean> => {
    if (!pendingActionRef.current) {
      console.log("‚ö†Ô∏è handlePendingActionConfirmation: Kh√¥ng c√≥ pending action")
      return false
    }

    const { isConfirmed, isRejected } = isConfirmation(transcript)
    const selectionIndex = parseSelectionIndex(transcript.toLowerCase())

    const pendingAction = pendingActionRef.current
    const { contactName, message, content, contactId, directChatId, groupId, chatType } =
      pendingAction
    const rate = settings?.speechRate ?? 1.0
    // Backend sends recipientUserId (the actual userId of recipient)
    const recipientUserId =
      (pendingAction as any).recipientUserId || (pendingAction as any).contactUserId
    // Backend sends 'content' field, frontend may have 'message' field
    const messageContent = content || message || ""
    // Backend sends 'targetName', frontend may have 'contactName'
    const finalContactName = contactName || (pendingAction as any).targetName || ""

    console.log("üîç handlePendingActionConfirmation START:", {
      transcript,
      isConfirmed,
      isRejected,
      pendingActionType: pendingAction.type,
      contactId,
      recipientUserId,
      contactName,
      targetName: (pendingAction as any).targetName,
      finalContactName,
      message,
      content,
      messageContent,
      stickerId: (pendingAction as any).stickerId,
      stickerDescription: (pendingAction as any).stickerDescription,
      chatType,
      directChatId,
      groupId,
      fullPendingAction: pendingAction,
    })

    // Handle incoming call
    if (pendingAction.type === "incoming_call") {
      console.log("üìû Cu·ªôc g·ªçi ƒë·∫øn ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi backend")
      return true
    }

    // Handle attachment selection
    if (pendingActionRef.current?.type === "choose_attachment" && selectionIndex) {
      const pendingChoose = pendingActionRef.current
      const candidates = pendingChoose.attachmentCandidates || []
      if (selectionIndex >= 1 && selectionIndex <= candidates.length) {
        const chosen = candidates.find((c) => c.index === selectionIndex)
        if (chosen) {
          pendingActionRef.current = {
            type: pendingChoose.originalActionType as PendingAction["type"],
            contactName: pendingChoose.contactName,
            message: "",
            contactId: pendingChoose.contactId,
            directChatId: pendingChoose.directChatId,
            groupId: pendingChoose.groupId,
            chatType: pendingChoose.chatType,
            lastBotMessage: `B·∫°n mu·ªën g·ª≠i ${chosen.type.startsWith("image") ? "·∫£nh" : "file"} "${chosen.name}" cho ${pendingChoose.contactName}, ƒë√∫ng kh√¥ng?`,
            selectedAttachmentIndex: selectionIndex,
            attachmentKind: pendingChoose.attachmentKind,
            attachmentCandidates: candidates,
            originalActionType: pendingChoose.originalActionType as any,
          }
          isWaitingForConfirmationRef.current = true
          await speakText(
            `B·∫°n mu·ªën g·ª≠i ${chosen.type.startsWith("image") ? "·∫£nh" : "file"} "${chosen.name}" cho ${pendingChoose.contactName}, ƒë√∫ng kh√¥ng?`,
            settings?.speechRate ?? 1.0,
            true
          )
          return true
        }
      }
      await speakText(
        "S·ªë b·∫°n ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng n√≥i l·∫°i.",
        settings?.speechRate ?? 1.0,
        true
      )
      return true
    }

    // Handle unclear response
    if (!isConfirmed && !isRejected) {
      console.log("‚ö†Ô∏è Ng∆∞·ªùi d√πng n√≥i c√°i kh√°c - backend s·∫Ω h·ªèi l·∫°i, gi·ªØ l·∫°i pending state")
      console.log("‚ö†Ô∏è isConfirmed:", isConfirmed, "isRejected:", isRejected)
      return true
    }

    const pendingActionForHandlers = pendingActionRef.current

    console.log(
      "üìç Preparing to handle action, isConfirmed:",
      isConfirmed,
      "isRejected:",
      isRejected
    )

    // Handle confirmations for different action types
    if (isConfirmed) {
      // Send message
      if (pendingActionForHandlers?.type === "send_message") {
        // Backend sends recipientUserId (the actual recipient's userId for direct chat)
        const finalRecipientId = recipientUserId || contactId

        console.log("üîç send_message validation:", {
          recipientUserId,
          contactId,
          finalRecipientId,
          chatType,
          messageContent,
          hasContent: !!messageContent,
        })

        if (!finalRecipientId && chatType !== "group") {
          console.error("‚ùå Kh√¥ng t√¨m th·∫•y recipientUserId ƒë·ªÉ g·ª≠i message")
          await speakText("Kh√¥ng t√¨m th·∫•y th√¥ng tin ng∆∞·ªùi nh·∫≠n", rate, false)
          return false
        }

        if (!messageContent || messageContent.trim() === "") {
          console.error("‚ùå N·ªôi dung tin nh·∫Øn tr·ªëng")
          await speakText("Kh√¥ng c√≥ n·ªôi dung tin nh·∫Øn ƒë·ªÉ g·ª≠i", rate, false)
          return false
        }

        await handleSendMessage({
          contactId: finalRecipientId,
          contactName: finalContactName,
          chatType: chatType as any,
          directChatId,
          groupId,
          message: messageContent,
          rate,
          speakText,
          restartWakeWordDetection,
        })
        return false
      }

      // Send sticker
      console.log("üîç Checking send_sticker conditions:", {
        type: pendingAction.type,
        typeMatch: pendingAction.type === "send_sticker",
        isConfirmed,
        stickerId: pendingAction.stickerId,
        hasStickerIdFromPending: !!pendingAction.stickerId,
        allConditions:
          pendingAction.type === "send_sticker" && isConfirmed && pendingAction.stickerId,
      })

      if (pendingAction.type === "send_sticker" && isConfirmed && pendingAction.stickerId) {
        // Backend sends recipientUserId (the actual recipient's userId for direct chat)
        const finalRecipientId = recipientUserId || contactId

        console.log("‚úÖ ƒêi·ªÅu ki·ªán send_sticker th·ªèa m√£n - G·ªçi handleSendSticker", {
          contactId,
          recipientUserId,
          finalRecipientId,
          contactUserId: (pendingAction as any).contactUserId,
          targetId: (pendingAction as any).targetId,
          chatType,
          directChatId,
          groupId,
          stickerId: pendingAction.stickerId,
          fullPending: pendingAction,
        })

        if (!finalRecipientId && chatType !== "group") {
          console.error("‚ùå Kh√¥ng t√¨m th·∫•y recipientUserId ƒë·ªÉ g·ª≠i sticker")
          await speakText("Kh√¥ng t√¨m th·∫•y th√¥ng tin ng∆∞·ªùi nh·∫≠n", rate, false)
          return false
        }

        await handleSendSticker({
          contactId: finalRecipientId,
          contactName: finalContactName,
          chatType: chatType as any,
          directChatId,
          groupId,
          stickerId: pendingAction.stickerId,
          stickerDescription: pendingAction.stickerDescription,
          rate,
          speakText,
          restartWakeWordDetection,
        })
        return true
      }

      // Send emoji
      if (pendingAction.type === "send_emoji" && isConfirmed && pendingAction.emoji) {
        // Backend sends recipientUserId (the actual recipient's userId for direct chat)
        const finalRecipientId = recipientUserId || contactId

        console.log("üîç send_emoji validation:", {
          recipientUserId,
          contactId,
          finalRecipientId,
          chatType,
          emoji: pendingAction.emoji,
        })

        if (!finalRecipientId && chatType !== "group") {
          console.error("‚ùå Kh√¥ng t√¨m th·∫•y recipientUserId ƒë·ªÉ g·ª≠i emoji")
          await speakText("Kh√¥ng t√¨m th·∫•y th√¥ng tin ng∆∞·ªùi nh·∫≠n", rate, false)
          return false
        }

        await handleSendEmoji({
          contactId: finalRecipientId,
          contactName: finalContactName,
          chatType: chatType as any,
          directChatId,
          groupId,
          emoji: pendingAction.emoji,
          emojiDescription: pendingAction.emojiDescription,
          rate,
          speakText,
          restartWakeWordDetection,
        })
        return false
      }

      // Create group
      if (
        pendingAction.type === "create_group" &&
        isConfirmed &&
        pendingAction.groupName &&
        pendingAction.memberIds
      ) {
        await handleCreateGroup({
          groupName: pendingAction.groupName,
          memberIds: pendingAction.memberIds,
          memberNames: pendingAction.memberNames,
          rate,
          speakText,
          restartWakeWordDetection,
          router,
        })
        return false
      }

      // Make call
      if (pendingAction.type === "make_call" && isConfirmed) {
        const finalRecipientId = recipientUserId || contactId

        console.log("üîç make_call validation:", {
          recipientUserId,
          contactId,
          finalRecipientId,
          chatType,
          directChatId,
          groupId,
          isVideoCall: (pendingAction as any).isVideo,
        })

        // Ki·ªÉm tra: n·∫øu l√† direct call c·∫ßn recipientId, n·∫øu l√† group call c·∫ßn groupId
        if (chatType === "direct" && !finalRecipientId) {
          console.error("‚ùå Kh√¥ng t√¨m th·∫•y recipientUserId ƒë·ªÉ g·ªçi tr·ª±c ti·∫øp")
          await speakText("Kh√¥ng t√¨m th·∫•y th√¥ng tin ng∆∞·ªùi nh·∫≠n", rate, false)
          return false
        }

        if (chatType === "group" && !groupId) {
          console.error("‚ùå Kh√¥ng t√¨m th·∫•y groupId ƒë·ªÉ g·ªçi nh√≥m")
          await speakText("Kh√¥ng t√¨m th·∫•y th√¥ng tin nh√≥m", rate, false)
          return false
        }

        await handleMakeCall({
          contactId: finalRecipientId,
          contactName: finalContactName,
          chatType: chatType as any,
          directChatId,
          groupId,
          isVideoCall: (pendingAction as any).isVideo,
          rate,
          speakText,
          restartWakeWordDetection,
        })
        return false
      }

      // Invite to group
      if (pendingAction.type === "invite_to_group" && isConfirmed) {
        console.log("üë• [CONFIRMATION] Handling invite_to_group confirmation:", {
          groupId: pendingAction.groupId || (pendingAction as any).targetId,
          memberIds: pendingAction.memberIds || (pendingAction as any).memberIds,
          memberNames: pendingAction.memberNames,
          groupName: pendingAction.groupName || (pendingAction as any).targetName,
        })

        // Extract data t·ª´ pending action
        const groupId = pendingAction.groupId || (pendingAction as any).targetId
        const memberIds = pendingAction.memberIds || []
        const memberNames = pendingAction.memberNames || []
        const groupName = pendingAction.groupName || (pendingAction as any).targetName || "nh√≥m"

        if (!groupId || !memberIds.length) {
          console.error("‚ùå Kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ m·ªùi v√†o nh√≥m", { groupId, memberIds })
          await speakText("Kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·ªÉ m·ªùi v√†o nh√≥m.", rate, false)
          return false
        }

        try {
          console.log("üîÑ [CONFIRMATION] Calling groupMemberService.addMembersToGroupChat...")
          const result = await groupMemberService.addMembersToGroupChat(groupId, memberIds)
          console.log("‚úÖ [CONFIRMATION] API Response:", result)

          const memberNamesStr =
            memberNames.length > 0 ? memberNames.join(", ") : `${memberIds.length} th√†nh vi√™n`
          const successMsg = `ƒê√£ m·ªùi ${memberNamesStr} v√†o nh√≥m ${groupName} th√†nh c√¥ng.`
          console.log("üé§ [CONFIRMATION] Speaking:", successMsg)

          await speakText(successMsg, rate, false)
          console.log("‚úÖ [CONFIRMATION] Invite to group completed")
        } catch (err) {
          console.error("‚ùå [CONFIRMATION] Error inviting to group:", err)
          await speakText("L·ªói khi m·ªùi v√†o nh√≥m. Vui l√≤ng th·ª≠ l·∫°i.", rate, false)
        }
        return true
      }

      // TODO: Add more action handlers (send_image, send_document, etc.)
      // These would follow similar patterns
    }

    // Handle rejection
    if (isRejected) {
      const { getActionName } = await import("../utils/confirmation")
      const actionType = getActionName(pendingAction.type)
      console.log(`‚ùå User t·ª´ ch·ªëi - h·ªßy ${actionType}`)
      await speakText(`ƒê√£ h·ªßy ${actionType}.`, rate, false)
      setTimeout(() => restartWakeWordDetection(), 500)
      return false
    }

    return true
  }

  // Main recording function
  const startRecordingAndSend = async () => {
    console.log(
      "üìç startRecordingAndSend called, isWaitingForConfirmation:",
      isWaitingForConfirmationRef.current
    )

    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      console.log("‚ö†Ô∏è ƒêang recording - D·ª´ng recording c≈© ƒë·ªÉ b·∫Øt ƒë·∫ßu m·ªõi")
      mediaRecorderRef.current.stop()
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((t) => t.stop())
        streamRef.current = null
      }
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
        silenceTimerRef.current = null
      }
      if (maxRecordingTimerRef.current) {
        clearTimeout(maxRecordingTimerRef.current)
        maxRecordingTimerRef.current = null
      }
      await new Promise((resolve) => setTimeout(resolve, 200))
    }

    if (!settings) {
      console.log("‚ö†Ô∏è Skip: ch∆∞a c√≥ settings")
      return
    }

    try {
      console.log("üî¥ B·∫Øt ƒë·∫ßu ghi √¢m...")
      setIsRecording(true)

      if (audioContextRef.current && !isWaitingForConfirmationRef.current) {
        await audioContextRef.current.suspend()
        console.log("‚è∏Ô∏è ƒê√£ t·∫°m d·ª´ng Porcupine")
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
        },
      })

      streamRef.current = stream

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: "audio/webm;codecs=opus",
      })
      mediaRecorderRef.current = mediaRecorder
      const chunks: Blob[] = []

      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunks.push(e.data)
        }
      }

      mediaRecorder.onstop = async () => {
        const blob = new Blob(chunks, { type: "audio/webm" })
        console.log("üì¶ mediaRecorder.onstop called, blob size:", blob.size)

        if (blob.size === 0) {
          console.warn("‚ö†Ô∏è Audio blob r·ªóng!")
        }

        const base64 = await blobToBase64(blob)
        setStatus("‚è≥ ƒêang x·ª≠ l√Ω l·ªánh...")

        try {
          const audioData = base64.split(",")[1]
          console.log("üì§ G·ª≠i audio data, length:", audioData.length)

          if (!isWaitingForConfirmationRef.current) {
            console.log("üíæ L∆∞u audio v√†o lastAudioDataRef")
            lastAudioDataRef.current = audioData
          }

          const response = await sendVoiceCommand(audioData)
          console.log("üì• Response t·ª´ backend:", response)
          console.log("üì• Response.pending:", (response as any).pending)
          console.log("üì• Response.needsConfirmation:", response.needsConfirmation)
          console.log("üì• Response.transcript:", response.transcript)

          // Store pending action before backend might clear it
          const hadPendingAction = pendingActionRef.current
          const wasWaitingForConfirmation = isWaitingForConfirmationRef.current
          console.log("üì• Current state:", {
            hadPendingAction: !!hadPendingAction,
            pendingActionType: hadPendingAction?.type,
            wasWaitingForConfirmation,
            isWaitingForConfirmationRefCurrent: isWaitingForConfirmationRef.current,
          })

          console.log("üîç Ki·ªÉm tra confirmation:", {
            hadPendingAction: !!hadPendingAction,
            wasWaitingForConfirmation,
            needsConfirmation: response.needsConfirmation,
            transcript: response.transcript,
          })

          // Handle pending action confirmation FIRST (before updating pending state)
          // If we were waiting for confirmation, always handle it regardless of backend response
          if (hadPendingAction && wasWaitingForConfirmation) {
            console.log("üìã ƒêang ch·ªù x√°c nh·∫≠n, x·ª≠ l√Ω x√°c nh·∫≠n t·ª´ user...")
            const confirmationHandled = await handlePendingActionConfirmation(
              response.transcript || ""
            )

            if (confirmationHandled) {
              console.log("‚úÖ Confirmation handled successfully")
              // Clear pending after successful handling
              pendingActionRef.current = null
              isWaitingForConfirmationRef.current = false
              isWakeWordProcessingRef.current = false // üîì Unlock - c√≥ th·ªÉ nh·∫≠n wake word ti·∫øp

              // ‚ö†Ô∏è DON'T return yet! Backend may send clientAction after confirmation
              // Continue to check for clientAction below
            } else {
              console.log("‚ö†Ô∏è Confirmation not handled - continuing with normal flow")
            }
          }

          // Update pending state from backend response AFTER handling
          if ((response as any).pending !== undefined) {
            if ((response as any).pending === null) {
              console.log("‚úÖ Backend cleared pending")
              pendingActionRef.current = null
              isWaitingForConfirmationRef.current = false
            } else {
              console.log("üìù Backend updated pending:", (response as any).pending)
              pendingActionRef.current = (response as any).pending
            }
          } // Handle clientAction from backend
          if ((response as any).clientAction) {
            console.log("üìã [MAIN] clientAction detected, calling handleClientAction...")
            const handled = await handleClientAction((response as any).clientAction, {
              speakText,
              restartWakeWordDetection,
              router,
              settings: { speechRate: settings.speechRate },
              pendingActionRef,
              isWaitingForConfirmationRef,
            })
            if (handled) {
              console.log("‚úÖ [MAIN] clientAction handled successfully, unlocking wake word...")
              isWakeWordProcessingRef.current = false // üîì Unlock after clientAction
              return
            }
          }

          // Handle response text
          if (response.response) {
            console.log("ü§ñ Tr·ª£ l√Ω:", response.response)
            setStatus(`üí¨ ${response.response.substring(0, 50)}...`)

            const isUserTranscript =
              response.response === response.transcript ||
              response.response.includes(response.transcript || "")

            if (!isUserTranscript) {
              if (response.needsConfirmation) {
                isWaitingForConfirmationRef.current = true
                await speakText(response.response, settings.speechRate, true)
              } else {
                await speakText(response.response, settings.speechRate, false)
                setTimeout(() => restartWakeWordDetection(), 500)
              }
            } else {
              setTimeout(() => restartWakeWordDetection(), 500)
            }
          } else {
            setTimeout(() => {
              if (settings.voiceActivationMode === "WAKE_WORD") {
                restartWakeWordDetection()
              }
            }, 2000)
          }
        } catch (err: unknown) {
          const error = err as Error
          console.error("‚ùå L·ªói g·ªçi API:", error)
          setStatus(`‚ùå ${error.message}`)
          await speakText(`C√≥ l·ªói x·∫£y ra: ${error.message}`, settings.speechRate)
          isWakeWordProcessingRef.current = false // üîì Unlock on error
          setTimeout(() => {
            if (settings.voiceActivationMode === "WAKE_WORD") {
              restartWakeWordDetection()
            }
          }, 3000)
        }

        stream.getTracks().forEach((t) => t.stop())
        streamRef.current = null
        setIsRecording(false)

        if (!isWaitingForConfirmationRef.current) {
          isWakeWordProcessingRef.current = false // üîì Unlock n·∫øu ko waiting confirmation
          await restartWakeWordDetection()
        }
      }

      // Setup VAD
      const audioContext = new AudioContext()
      const source = audioContext.createMediaStreamSource(stream)
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 2048
      analyser.smoothingTimeConstant = 0.3
      source.connect(analyser)

      const isConfirmationMode = isWaitingForConfirmationRef.current
      const vadState = createVADState()
      const thresholds = getVADThresholds(isConfirmationMode)

      const checkAudioLevel = () => {
        if (mediaRecorder.state !== "recording") {
          console.log("‚ö†Ô∏è mediaRecorder kh√¥ng c√≤n recording state")
          audioContext.close()
          return
        }

        const vadResult = calculateSpeechProbability(
          { isConfirmationMode, audioContext, analyser },
          vadState,
          thresholds
        )

        const now = Date.now()

        // Grace period
        if (now - vadState.recordingStartTime < thresholds.STARTUP_GRACE_PERIOD) {
          if (
            Math.floor((now - vadState.recordingStartTime) / 500) !==
            Math.floor((now - vadState.recordingStartTime - 50) / 500)
          ) {
            console.log(`‚è≥ Grace period: ${now - vadState.recordingStartTime}ms`)
          }
          silenceTimerRef.current = setTimeout(checkAudioLevel, 50)
          return
        }

        // Update VAD state
        updateVADState(vadState, vadResult.isSpeech, thresholds)

        // Log speech detection
        if (vadResult.isSpeech && !vadState.hasSpoken) {
          console.log(`üó£Ô∏è Ph√°t hi·ªán gi·ªçng n√≥i (nƒÉng l∆∞·ª£ng: ${vadResult.energy.toFixed(1)})`)
          setStatus("üé§ ƒêang nghe...")
        }

        // Check if should stop
        const stopResult = shouldStopRecording(vadState, thresholds)
        if (stopResult.shouldStop) {
          console.log(`‚úÖ D·ª´ng ghi √¢m: ${stopResult.reason}`)
          mediaRecorder.stop()
          audioContext.close()
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current)
            silenceTimerRef.current = null
          }
          if (maxRecordingTimerRef.current) {
            clearTimeout(maxRecordingTimerRef.current)
            maxRecordingTimerRef.current = null
          }
          return
        }

        silenceTimerRef.current = setTimeout(checkAudioLevel, 50)
      }

      mediaRecorder.start()
      setStatus("üî¥ ƒêang ch·ªù b·∫°n n√≥i...")
      console.log("üéôÔ∏è B·∫Øt ƒë·∫ßu ghi √¢m v·ªõi VAD")
      checkAudioLevel()

      maxRecordingTimerRef.current = setTimeout(() => {
        if (mediaRecorder.state === "recording") {
          console.log(`‚è±Ô∏è ƒê·∫†T TIMEOUT - D·ª´ng ghi √¢m`)
          mediaRecorder.stop()
          audioContext.close()
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current)
            silenceTimerRef.current = null
          }
        }
      }, thresholds.MAX_RECORDING_TIME)
    } catch (err: unknown) {
      const error = err as Error
      console.error("‚ùå L·ªói mic:", error)
      setStatus("‚ùå Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c mic")
      setIsRecording(false)

      if (audioContextRef.current && audioContextRef.current.state === "suspended") {
        await audioContextRef.current.resume()
      }

      if (streamRef.current) {
        streamRef.current.getTracks().forEach((t) => t.stop())
        streamRef.current = null
      }

      setTimeout(() => {
        restartWakeWordDetection()
      }, 1000)
    }
  }

  // Initialize Porcupine wake word detection
  useEffect(() => {
    if (!settings) return

    if (settings.voiceActivationMode !== "WAKE_WORD" || !settings.sttEnabled) {
      cleanup()
      setStatus("üí§ Tr·ª£ l√Ω t·∫Øt (ch·∫ø ƒë·ªô gi·ªØ n√∫t)")
      return
    }

    const initPorcupine = async () => {
      try {
        setStatus("‚è≥ ƒêang t·∫£i wake word model...")

        const accessKey = process.env.NEXT_PUBLIC_PICOVOICE_ACCESS_KEY || "YOUR_ACCESS_KEY"
        if (!accessKey || accessKey === "YOUR_ACCESS_KEY") {
          throw new Error("Thi·∫øu NEXT_PUBLIC_PICOVOICE_ACCESS_KEY")
        }

        const keywords = [
          {
            publicPath: "/models/hey-chat_en_wasm_v3_0_0.ppn",
            label: "hey-chat",
            sensitivity: 0.9,
          },
        ]

        console.log("‚è≥ T·∫°o Porcupine worker...")

        const worker = await PorcupineWorker.create(
          accessKey,
          keywords,
          async (detection: PorcupineDetection) => {
            lastWakeWordDetectionTimeRef.current = Date.now()

            console.log("üé§üé§üé§ WAKE WORD DETECTED! üé§üé§üé§", detection)

            // üîí N·∫øu ƒëang x·ª≠ l√Ω l·ªánh t·ª´ wake word tr∆∞·ªõc, b·ªè qua
            if (isWakeWordProcessingRef.current) {
              console.log("üîí ƒêang x·ª≠ l√Ω l·ªánh, b·ªè qua detection")
              return
            }

            // Handle incoming call
            if (
              pendingActionRef.current?.type === "incoming_call" &&
              !isWaitingForConfirmationRef.current
            ) {
              console.log("üìû C√≥ incoming call - ghi √¢m ph·∫£n h·ªìi")
              setStatus(`‚ú® Nghe th·∫•y "${settings.wakeWordPhrase}"!`)
              await speakText("B·∫°n c·∫ßn t√¥i l√†m g√¨?", settings.speechRate, false)
              isWaitingForConfirmationRef.current = true
              isWakeWordProcessingRef.current = true // üîí Lock
              setTimeout(() => {
                console.log("üìû B·∫Øt ƒë·∫ßu ghi √¢m sau 500ms delay")
                startRecordingAndSend()
              }, 500)
              return
            }

            // Handle confirmation mode
            if (isWaitingForConfirmationRef.current) {
              console.log("üìù Khi ch·ªù x√°c nh·∫≠n - ghi √¢m c√¢u tr·∫£ l·ªùi")
              startRecordingAndSend()
              return
            }

            // Normal flow
            isWakeWordProcessingRef.current = true // üîí Lock khi detect
            setStatus(`‚ú® Nghe th·∫•y "${settings.wakeWordPhrase}"!`)
            await speakText("B·∫°n c·∫ßn t√¥i l√†m g√¨?", settings.speechRate, false)
            setTimeout(() => {
              console.log("üéôÔ∏è B·∫Øt ƒë·∫ßu ghi √¢m sau 800ms delay")
              startRecordingAndSend()
            }, 800)
          },
          { publicPath: "/models/porcupine_params.pv" }
        )

        console.log("‚úÖ Porcupine worker t·∫°o th√†nh c√¥ng")
        workerRef.current = worker

        await startMicrophoneForWakeWord(worker)
      } catch (err: unknown) {
        const error = err as Error
        console.error("‚ùå Kh·ªüi ƒë·ªông Porcupine th·∫•t b·∫°i:", error)
        setStatus(`‚ùå L·ªói: ${error.message}`)
      }
    }

    initPorcupine()

    return () => {
      cleanup()
    }
  }, [settings?.voiceActivationMode, settings?.sttEnabled, settings?.wakeWordPhrase])

  // Monitor wake word detection heartbeat
  useEffect(() => {
    if (detectionHeartbeatRef.current) {
      clearInterval(detectionHeartbeatRef.current)
    }

    detectionHeartbeatRef.current = setInterval(() => {
      const now = Date.now()
      const timeSinceLastDetection = now - lastWakeWordDetectionTimeRef.current

      const DETECTION_TIMEOUT = 30000
      if (timeSinceLastDetection > DETECTION_TIMEOUT && isListening) {
        console.warn(`‚ö†Ô∏è Wake word detection stuck - Restarting...`)
        setStatus("üîÑ Restarting wake word detection...")
        restartWakeWordDetection().catch((err) => {
          console.error("‚ùå Failed to restart detection:", err)
          setStatus("‚ùå Failed to restart detection")
        })
        lastWakeWordDetectionTimeRef.current = now
      }
    }, 10000)

    return () => {
      if (detectionHeartbeatRef.current) {
        clearInterval(detectionHeartbeatRef.current)
      }
    }
  }, [isListening])

  // Keyboard shortcut: Ctrl+V to reset
  useEffect(() => {
    const resetMic = async () => {
      console.log("‚å®Ô∏è Ctrl+V pressed - Reset mic")

      pendingActionRef.current = null
      isWaitingForConfirmationRef.current = false
      lastAudioDataRef.current = null

      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
        silenceTimerRef.current = null
      }
      if (maxRecordingTimerRef.current) {
        clearTimeout(maxRecordingTimerRef.current)
        maxRecordingTimerRef.current = null
      }

      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
        mediaRecorderRef.current.stop()
      }

      setIsRecording(false)
      setStatus("üîÑ ƒêang reset mic...")

      try {
        // Reset Redis pending on backend
        console.log(" G·ªçi API reset-pending tr√™n backend...")
        await pushNotificationService.resetVoiceAssistantPending()
        console.log(" Redis pending ƒë√£ ƒë∆∞·ª£c reset tr√™n backend")
      } catch (err) {
        console.error(" L·ªói g·ªçi API reset-pending:", err)
      }

      try {
        if (workerRef.current) {
          if (audioContextRef.current && audioContextRef.current.state === "running") {
            await audioContextRef.current.close()
            audioContextRef.current = null
          }
          if (micStreamRef.current) {
            micStreamRef.current.getTracks().forEach((t) => t.stop())
            micStreamRef.current = null
          }

          await startMicrophoneForWakeWord(workerRef.current)
        }
      } catch (err) {
        console.error("‚ùå L·ªói khi reset detection:", err)
        setStatus("‚ùå L·ªói khi reset mic")
      }
    }

    const handleKeyPress = (e: KeyboardEvent) => {
      if ((e.ctrlKey || e.metaKey) && e.key.toLowerCase() === "v") {
        e.preventDefault()
        resetMic().catch((err) => console.error("‚ùå L·ªói reset mic:", err))
      }
    }

    window.addEventListener("keydown", handleKeyPress)
    return () => {
      window.removeEventListener("keydown", handleKeyPress)
    }
  }, [settings?.wakeWordPhrase])

  // Stop recording function
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.stop()
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((t) => t.stop())
      streamRef.current = null
    }
    setIsRecording(false)
  }

  return {
    status,
    isListening,
    isRecording,
    settings,
    startRecordingAndSend,
    stopRecording,
  }
}
